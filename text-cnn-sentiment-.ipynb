{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.data import Dataset, Iterator\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import datetime\n",
    "\n",
    "###### Do not modify here ###### \n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = graph_def\n",
    "    #strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n",
    "###### Do not modify  here ######\n",
    "\n",
    "###### Do not modify here ###### \n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit, watch Rafa and Joh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I'm a GSP fan, i just hate Nick D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id lang  polarity  \\\n",
       "0  264183816548130816   en  positive   \n",
       "1  263405084770172928   en  negative   \n",
       "2  262163168678248449   en  negative   \n",
       "3  264249301910310912   en  negative   \n",
       "4  264105751826538497   en  positive   \n",
       "\n",
       "                                               tweet  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...  \n",
       "1  Theo Walcott is still shit, watch Rafa and Joh...  \n",
       "2  its not that I'm a GSP fan, i just hate Nick D...  \n",
       "3  Iranian general says Israel's Iron Dome can't ...  \n",
       "4  with J Davlar 11th. Main rivals are team Polan...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# Load data\n",
    "en_full = pd.read_csv('data/supervised_phase/en_full/en_full.tsv.txt', delimiter='\\t', names=[\"id\", \"lang\", \"polarity\", \"tweet\"])\n",
    "en_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word2index(tweet):\n",
    "    \"\"\"\n",
    "    Convert a tweet to a sequence of word index \n",
    "    \"\"\"\n",
    "    return [vocabulary_dict.get(token)[0] for token in tknzr.tokenize(tweet.lower()) if vocabulary_dict.get(token) != None]\n",
    "\n",
    "def polarity2label(polarity):\n",
    "    if polarity =='negative':  return 0\n",
    "    elif polarity =='neutral': return 1\n",
    "    elif polarity =='positive':return 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>264183816548130816</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>Gas by my house hit $3.39!!!! I'm going to Cha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263405084770172928</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Theo Walcott is still shit, watch Rafa and Joh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262163168678248449</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>its not that I'm a GSP fan, i just hate Nick D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>264249301910310912</td>\n",
       "      <td>en</td>\n",
       "      <td>negative</td>\n",
       "      <td>Iranian general says Israel's Iron Dome can't ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>264105751826538497</td>\n",
       "      <td>en</td>\n",
       "      <td>positive</td>\n",
       "      <td>with J Davlar 11th. Main rivals are team Polan...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id lang  polarity  \\\n",
       "0  264183816548130816   en  positive   \n",
       "1  263405084770172928   en  negative   \n",
       "2  262163168678248449   en  negative   \n",
       "3  264249301910310912   en  negative   \n",
       "4  264105751826538497   en  positive   \n",
       "\n",
       "                                               tweet  label  \n",
       "0  Gas by my house hit $3.39!!!! I'm going to Cha...      2  \n",
       "1  Theo Walcott is still shit, watch Rafa and Joh...      0  \n",
       "2  its not that I'm a GSP fan, i just hate Nick D...      0  \n",
       "3  Iranian general says Israel's Iron Dome can't ...      0  \n",
       "4  with J Davlar 11th. Main rivals are team Polan...      2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en_full['index_rep'] = en_full['tweet'].map(word2index)\n",
    "en_full['label'] = en_full['polarity'].map(polarity2label)\n",
    "en_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import learn\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(tknzr.tokenize(tweet)) for tweet in  en_full['tweet']])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "tweets = np.array(list(vocab_processor.fit_transform(en_full['tweet'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training + Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16239, 42)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "rs = ShuffleSplit(n_splits=1, test_size=.1, random_state=0)\n",
    "\n",
    "senti = en_full['label'].as_matrix()\n",
    "\n",
    "for train_index, test_index in rs.split(senti):\n",
    "    X_train = tweets[train_index]\n",
    "    y_train = senti[train_index]\n",
    "\n",
    "    X_test = tweets[test_index]\n",
    "    y_test = senti[test_index]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1859185, 200)\n",
      "1859184\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load pre train Word2vec\n",
    "wb_matrix = np.load(\"data/embed_tweets_en_200M_200D/embedding_matrix.npy\")\n",
    "print(wb_matrix.shape)\n",
    "vocabulary_dict = pickle.load(open(\"data/embed_tweets_en_200M_200D/vocabulary.pickle\", \"rb\"))\n",
    "print(len(vocabulary_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134, 2480704)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_dict.get('am')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # initial matrix with random uniform\n",
    "initW = np.random.uniform(-0.25,0.25,(len(vocab_processor.vocabulary_), wb_matrix.shape[1]))\n",
    "# load any vectors from the word2vec\n",
    "for word, vector in vocabulary_dict.items():\n",
    "    idx = vocab_processor.vocabulary_.get(word)\n",
    "    if idx != 0:\n",
    "        initW[idx] = wb_matrix[vector[0]]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = max_document_length\n",
    "n_output = 3\n",
    "learning_rate = 0.0017\n",
    "\n",
    "embedding_size = initW.shape[1]\n",
    "filter_sizes = [4,3]\n",
    "num_filters = 200\n",
    "pooling_size = 4\n",
    "pooling_strides = 2\n",
    "epochs_num = 2000\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, shape = (None, n_input), name = \"Input_X\")\n",
    "y = tf.placeholder(tf.int32, shape = (None), name = \"Y\")\n",
    "# mode = tf.placeholder(tf.bool, name = \"Mode\")\n",
    "\n",
    "# Load Embedding Model\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    word2vec = tf.Variable(tf.constant(0.0, shape = initW.shape),\n",
    "                    trainable=False, name=\"word2vec\") # trainable=False, means not update these embeddings\n",
    "\n",
    "embedded_chars = tf.nn.embedding_lookup(word2vec, X)\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1) # ex: convert [[1,2]] to [[1],[2]], that is shape (2,) to (2,1)\n",
    "\n",
    "# 1st convolution layer\n",
    "conv1 = tf.layers.conv2d(embedded_chars_expanded, \n",
    "                         filters = num_filters, \n",
    "                         kernel_size = (filter_sizes[0], initW.shape[1]),\n",
    "                         strides = (1,1), \n",
    "                         kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                         activation = tf.nn.relu,\n",
    "                         name=\"Convolution_1st\"\n",
    "                        )\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[4, 1], strides=2)\n",
    "\n",
    "# 2nd convolution layer\n",
    "conv2 = tf.layers.conv2d(pool1, \n",
    "                         filters = num_filters, \n",
    "                         kernel_size = (filter_sizes[1], 1),\n",
    "                         strides = (1,1), \n",
    "                         kernel_initializer=tf.contrib.layers.variance_scaling_initializer(),\n",
    "                         activation = tf.nn.relu,\n",
    "                         name=\"Convolution_2nd\"\n",
    "                        )\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[4, 1], strides=2)\n",
    "\n",
    "# Dense Layer, Combine all the pooled features\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7*200])\n",
    "\n",
    "dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu, name = \"Fully_connect\")\n",
    "# dense = tf.layers.dropout(inputs = dense, rate = 0.3, training = mode)\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=dense, units=n_output, activation=tf.nn.softmax, name = \"Softmax\")\n",
    "\n",
    "# Define Loss Function\n",
    "cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name=\"Cross_Entropy\"))\n",
    "\n",
    "# Define Training Process\n",
    "train_step = tf.train.AdadeltaOptimizer(0.0017).minimize(cross_entropy)\n",
    "\n",
    "# Define Accuracy\n",
    "predicted_class = tf.argmax(logits,1, output_type=tf.int32)\n",
    "correct_predict = tf.equal(y, predicted_class) # [True, False ..., True]\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32)) # [True, False ..., True] --> [1,0,...,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with tf.Session() as sess:\n",
    "#     show_graph(tf.get_default_graph().as_graph_def())\n",
    "#     # Initialize all variables\n",
    "#     sess.run(tf.local_variables_initializer())\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     sess.run(word2vec.assign(initW))# Assign the pretrain word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare the training batch\n",
    "train_data = tf.contrib.data.Dataset.from_tensor_slices((X_train,y_train)).batch(batch_size).repeat()\n",
    "train_iterator = train_data.make_one_shot_iterator() # Create an iterator to go through the training data\n",
    "train_next_batch = train_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yenhao/Documents/chatbot/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-11-14 23:28:56 - 1 epoch, train f1 score:0.231, test f1 score:0.221\n",
      "\n",
      "2017-11-14 23:29:31 - 26 epoch, train f1 score:0.391, test f1 score:0.349\n",
      "\n",
      "2017-11-14 23:30:07 - 51 epoch, train f1 score:0.385, test f1 score:0.359\n",
      "\n",
      "2017-11-14 23:30:44 - 76 epoch, train f1 score:0.370, test f1 score:0.369\n",
      "\n",
      "2017-11-14 23:31:20 - 101 epoch, train f1 score:0.443, test f1 score:0.375\n",
      "\n",
      "2017-11-14 23:31:55 - 126 epoch, train f1 score:0.415, test f1 score:0.384\n",
      "\n",
      "2017-11-14 23:32:31 - 151 epoch, train f1 score:0.395, test f1 score:0.385\n",
      "\n",
      "2017-11-14 23:33:07 - 176 epoch, train f1 score:0.411, test f1 score:0.390\n",
      "\n"
     ]
    }
   ],
   "source": [
    "round_of_epochs = int(X_train.shape[0]/batch_size)\n",
    "\n",
    "saver = tf.train.Saver() # to store the model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    sess.run(word2vec.assign(initW))# Assign the pretrain word2vec\n",
    "    \n",
    "    for epochs in range(epochs_num): # starting the training process and set the epochs_num\n",
    "        for _ in range(round_of_epochs):\n",
    "            train, label = sess.run(train_next_batch) # Get the mini-batch data sample\n",
    "            sess.run(train_step, feed_dict={X:train, y:label}) # Feed the features, labe, training_mode  to network to train\n",
    "        \n",
    "        if epochs % 25 ==0:\n",
    "            loss, pred = sess.run([cross_entropy,predicted_class], feed_dict={X:train, y:label})\n",
    "            print(\"{} - {} epoch, loss:{}, train f1 score:{:.3f}, test f1 score:{:.3f}\\n\".format(\n",
    "                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                epochs+1,\n",
    "                loss, \n",
    "                f1_score(label, pred, average='macro'),\n",
    "                f1_score(y_test, sess.run(predicted_class, feed_dict={X:X_test, y:y_test}), average='macro')\n",
    "                )\n",
    "            )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
